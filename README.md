# Awesome-Federated-LLM-Related-Works

## Table of Content

- [Black-Box Prompting](#Black-Box-Prompting)
- [Hallucination/Knowledge Enhanced/Chain of Thought](#Hallucination/Knowledge-Enhanced/Chain-of-Thought)
- [In-Context Learning Related](#In-Context-Learning-Related)

## Black-Box Prompting
- [2023/10] **EFFICIENT FEDERATED PROMPT TUNING FOR BLACK-BOX LARGE PRE-TRAINED MODELS** *Zihao Lin et al. arxiv.* [[paper](https://arxiv.org/abs/2310.03123)]
  - This work proposes Fed-BBPT, which federatedly trains prompt generators for users to employ PTMs without requiring knowledge of model architectures or parameters.

- [2023/10] **FEDBPT: EFFICIENT FEDERATED BLACK-BOX PROMPT TUNING FOR LARGE LANGUAGE MODELS** *Jingwei Sun et al. arxiv.* [[paper](https://arxiv.org/abs/2310.01467)]

- [2023/09] **LANGUAGE MODELS AS BLACK-BOX OPTIMIZERS FOR VISION-LANGUAGE MODELS** *Shihong Liu et al. arxiv.* [[paper](https://arxiv.org/abs/2309.05950)]

- [2023/08] **Gradient-Free Textual Inversion** *Zhengcong Fei et al. arxiv.* [[paper](https://arxiv.org/abs/2304.05818)]
  
- [2023/06] **Learning to Learn from APIs: Black-Box Data-Free Meta-Learning** *Zixuan Hu et al. IJCAI 2023.* [[paper](https://arxiv.org/abs/2305.18413)][[code](https://github.com/Egg-Hu/BiDf-MKD)]
  
- [2023/03] **BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning** *Changdae Oh et al. CVPR2023.* [[paper](https://arxiv.org/abs/2303.14773)] [[code](https://github.com/changdaeoh/BlackVIP)]
  - This work proposes BlackVIP, which efficiently adapts the PTM without knowledge about model architectures and parameters.
  
- [2022/01] **Black-Box Prompt Learning for Pre-trained Language Models** *Shizhe Diao et al. TMLR.* [[paper](https://arxiv.org/abs/2201.08531)][[code]( https://github.com/shizhediao/Black-Box-Prompt-Learning)]

- [2022/01] **Black-Box Tuning for Language-Model-as-a-Service** *Tianxiang Sun et al. ICML 2022.* [[paper](https://arxiv.org/abs/2201.03514)]

- [2022/01] **Black-box Prompt Tuning for Vision-Language Model as a Service** *Lang Yu et al. IJCAI 2023.* [[paper](https://www.ijcai.org/proceedings/2023/0187.pdf)][[code](https://github.com/BruthYU/BPT-VLM)]

## Hallucination/Knowledge Enhanced/Chain of Thought
- [2023/05] **Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback** *Baolin Peng et al. arxiv.* [[paper](https://arxiv.org/abs/2302.12813)][[code](https://github.com/feizc/Gradient-Free-Textual-Inversion)]
- [2023/10] **ZERO-RESOURCE HALLUCINATION PREVENTION FOR LARGE LANGUAGE MODELS** *Junyu Luo et al. arxiv.* [[paper](https://arxiv.org/abs/2309.02654)][[code](https://github.com/soap117/Self-evaluation)]
  
- [2023/09] **CRITIC: LARGE LANGUAGE MODELS CAN SELF-CORRECT WITH TOOL-INTERACTIVE CRITIQUING** *Zhibin Gou et al. arxiv.* [[paper](https://arxiv.org/abs/2305.11738)][[code](https://github.com/microsoft/ProphetNet/tree/master/CRITIC)]
  
- [2023/09] **MAKING LARGE LANGUAGE MODELS BETTER REASONERS WITH ALIGNMENT** *Peiyi Wang et al. arxiv.* [[paper](https://arxiv.org/abs/2309.02144)]
  
- [2023/08] **Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA** *Yuhan Ma et al. arxiv.* [[paper](https://arxiv.org/abs/2308.04679)]

- [2023/06] **Unifying Large Language Models and Knowledge Graphs: A Roadmap** *Shirui Pan et al. arxiv.* [[paper](https://arxiv.org/abs/2306.08302)]
  
- [2023/05] **COOK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge** *Shangbin Feng et al. arxiv.* [[paper](https://arxiv.org/abs/2305.09955)]
  
- [2023/05] **Augmented Large Language Models with Parametric Knowledge Guiding** *Ziyang Luo et al. arxiv.* [[paper](https://arxiv.org/abs/2305.04757)]

- [2023/05] **THINK-ON-GRAPH: DEEP AND RESPONSIBLE REASONING OF LARGE LANGUAGE MODEL ON KNOWLEDGE GRAPH** *Jiashuo Sun et al. arxiv.* [[paper](https://arxiv.org/abs/2307.07697)][[code](https://github.com/GasolSun36/ToG)]

- [2023/05] **Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering** *Xiangyang Liu et al. arxiv.* [[paper](https://arxiv.org/abs/2304.13911)]

## In-Context Learning Related
- [2023/05] **Can We Edit Factual Knowledge by In-Context Learning?** *Ce Zheng et al. IJCAI 2023.* [[paper](https://arxiv.org/abs/2305.12740)][[code](https://github.com/PKUnlp-icler/IKE)]

## Privacy
- [2023/05] **Can Public Large Language Models Help Private Cross-device Federated Learning?** *Boxin Wang et al. arxiv.* [[paper](https://arxiv.org/abs/2305.12132)]

## Distillation
- [2023/08] **Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes** *Cheng-Yu Hsieh et al. arxiv.* [[paper](https://arxiv.org/abs/2305.02301)][[code](https://github.com/google-research/distilling-step-by-step)]
- 
- [2023/08] **UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition** *Wenxuan Zhou et al. arxiv.* [[paper](https://arxiv.org/abs/2308.03279)][[page](https://universal-ner.github.io/)][[code](https://github.com/universal-ner/universal-ner)]
  
- [2023/05] **Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks** *Minki Kang et al. arxiv.* [[paper](https://arxiv.org/abs/2305.18395)]

## Prompting/Fine-tuning/Instruction Tuning
- [2023/10] **Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation** *Chen Dun et al. arxiv.* [[paper](https://arxiv.org/abs/2310.02842)]

- [2023/08] **Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation** *Fu-En Yang et al. ICCV 2023.* [[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Efficient_Model_Personalization_in_Federated_Learning_via_Client-Specific_Prompt_Generation_ICCV_2023_paper.html)]

- [2023/08] **FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning** *Haokun Chen et al. arxiv.* [[paper](https://arxiv.org/abs/2308.12305)]

- [2023/08] **FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models** *Pengwei Xing et al. arxiv.* [[paper](https://arxiv.org/abs/2308.15324)]

- [2023/08] **SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models** *Sara Babakniya et al. arxiv.* [[paper](https://arxiv.org/abs/2308.06522)]

- [2023/07] **Low-Parameter Federated Learning with Large Language Models** *Jingang Jiang et al. arxiv.* [[paper](https://arxiv.org/abs/2307.13896)]

- [2023/05] **Towards Building the Federated GPT: Federated Instruction Tuning** *Jianyi Zhang et al. arxiv.* [[paper](https://arxiv.org/abs/2305.05644)]

- [2023/05] **Instruction Tuned Models are Quick Learners** *Himanshu Gupta et al. arxiv.* [[paper](https://arxiv.org/abs/2306.05539)]

- [2023/02] **Offsite-Tuning: Transfer Learning without Full Model** *Guangxuan Xiao et al. arxiv.* [[paper](https://arxiv.org/abs/2302.04870)]

## Multi-Modal
- [2023/10] **MMICL: EMPOWERING VISION-LANGUAGE MODEL WITH MULTI-MODAL IN-CONTEXT LEARNING** *Haozhe Zhao et al. arxiv.* [[paper](https://arxiv.org/abs/2309.07915)][[code](https://github.com/PKUnlp-icler/MIC)]

- [2023/05] **DC-CCL: Device-Cloud Collaborative Controlled Learning for Large Vision Models** *Yucheng Ding et al. arxiv.* [[paper](https://arxiv.org/abs/2303.10361)]
