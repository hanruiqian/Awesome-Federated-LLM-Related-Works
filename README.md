# Awesome-Federated-LLM-Related-Works

## Table of Content

- [Black-Box Prompting](#Black-Box-Prompting)
- [Hallucination/Knowledge Enhanced/Chain of Thought](#Hallucination/Knowledge-Enhanced/Chain-of-Thought)
- [In-Context Learning Related](#In-Context-Learning-Related)

## Black-Box Prompting
- [2023/03] **BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning** *Changdae Oh et al. CVPR2023.* [[paper](https://arxiv.org/abs/2303.14773)] [[code](https://github.com/changdaeoh/BlackVIP)]
  - This work proposes BlackVIP, which efficiently adapts the PTM without knowledge about model architectures and parameters.
 
- [2023/10] **EFFICIENT FEDERATED PROMPT TUNING FOR BLACK-BOX LARGE PRE-TRAINED MODELS** *Zihao Lin et al. arxiv.* [[paper](https://arxiv.org/abs/2310.03123)]
  - This work proposes Fed-BBPT, which federatedly trains prompt generators for users to employ PTMs without requiring knowledge of model architectures or parameters.

- [2023/10] **FEDBPT: EFFICIENT FEDERATED BLACK-BOX PROMPT TUNING FOR LARGE LANGUAGE MODELS** *Jingwei Sun et al. arxiv.* [[paper](https://arxiv.org/abs/2310.01467)]

- [2022/01] **Black-Box Prompt Learning for Pre-trained Language Models** *Shizhe Diao et al. TMLR.* [[paper](https://arxiv.org/abs/2201.08531)][[code]( https://github.com/shizhediao/Black-Box-Prompt-Learning)]

- [2022/01] **Black-Box Tuning for Language-Model-as-a-Service** *Tianxiang Sun et al. ICML 2022.* [[paper](https://arxiv.org/abs/2201.03514)]

- [2022/01] **Black-box Prompt Tuning for Vision-Language Model as a Service** *Lang Yu et al. IJCAI 2023.* [[paper](https://www.ijcai.org/proceedings/2023/0187.pdf)][[code](https://github.com/BruthYU/BPT-VLM)]

- [2023/06] **Learning to Learn from APIs: Black-Box Data-Free Meta-Learning** *Zixuan Hu et al. IJCAI 2023.* [[paper](https://arxiv.org/abs/2305.18413)][[code](https://github.com/Egg-Hu/BiDf-MKD)]

- [2023/08] **Gradient-Free Textual Inversion** *Zhengcong Fei et al. IJCAI 2023.* [[paper](https://arxiv.org/abs/2304.05818)][[code](https://github.com/feizc/Gradient-Free-Textual-Inversion)]

- [2023/08] **Gradient-Free Textual Inversion** *Zhengcong Fei et al. arxiv.* [[paper](https://arxiv.org/abs/2304.05818)]

## Hallucination/Knowledge Enhanced/Chain of Thought
- [2023/05] **Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback** *Baolin Peng et al. arxiv.* [[paper](https://arxiv.org/abs/2302.12813)][[code](https://github.com/feizc/Gradient-Free-Textual-Inversion)]

- [2023/05] **COOK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge** *Shangbin Feng et al. arxiv.* [[paper](https://arxiv.org/abs/2305.09955)]

- [2023/09] **CRITIC: LARGE LANGUAGE MODELS CAN SELF-CORRECT WITH TOOL-INTERACTIVE CRITIQUING** *Zhibin Gou et al. arxiv.* [[paper](https://arxiv.org/abs/2305.11738)][[code](https://github.com/microsoft/ProphetNet/tree/master/CRITIC)]

- [2023/10] **ZERO-RESOURCE HALLUCINATION PREVENTION FOR LARGE LANGUAGE MODELS** *Junyu Luo et al. arxiv.* [[paper](https://arxiv.org/abs/2309.02654)][[code](https://github.com/soap117/Self-evaluation)]

- [2023/05] **Augmented Large Language Models with Parametric Knowledge Guiding** *Ziyang Luo et al. arxiv.* [[paper](https://arxiv.org/abs/2305.04757)]

- [2023/05] **THINK-ON-GRAPH: DEEP AND RESPONSIBLE REASONING OF LARGE LANGUAGE MODEL ON KNOWLEDGE GRAPH** *Jiashuo Sun et al. arxiv.* [[paper](https://arxiv.org/abs/2307.07697)][[code](https://github.com/GasolSun36/ToG)]

- [2023/05] **Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering** *Xiangyang Liu et al. arxiv.* [[paper](https://arxiv.org/abs/2304.13911)]

- [2023/06] **Unifying Large Language Models and Knowledge Graphs: A Roadmap** *Shirui Pan et al. arxiv.* [[paper](https://arxiv.org/abs/2306.08302)]

- [2023/08] **Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA** *Yuhan Ma et al. arxiv.* [[paper](https://arxiv.org/abs/2308.04679)]

## In-Context Learning Related
- [2023/05] **Can We Edit Factual Knowledge by In-Context Learning?** *Ce Zheng et al. IJCAI 2023.* [[paper](https://arxiv.org/abs/2305.12740)][[code](https://github.com/PKUnlp-icler/IKE)]

- [2023/05] **Can We Edit Factual Knowledge by In-Context Learning?** *Ce Zheng et al. IJCAI 2023.* [[paper](https://arxiv.org/abs/2305.12740)][[code](https://github.com/PKUnlp-icler/IKE)]
